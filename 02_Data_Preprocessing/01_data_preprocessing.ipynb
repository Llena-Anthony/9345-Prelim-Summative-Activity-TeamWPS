{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## BLOCK 0: Imports & Settings"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T16:28:58.933252500Z",
     "start_time": "2026-02-13T16:28:58.925681100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T16:29:00.061015900Z",
     "start_time": "2026-02-13T16:29:00.027451300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "print(\"CWD:\", Path.cwd())\n",
    "print(\"Files in CWD (first 30):\")\n",
    "for p in sorted(Path.cwd().glob(\"*\"))[:30]:\n",
    "    print(\" -\", p.name)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: D:\\Quelvin's Files\\DATA SCIENCE\\02_Data_Preprocessing\n",
      "Files in CWD (first 30):\n",
      " - 01_data_preprocessing.ipynb\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## BLOCK 1 — Robust project root & folders"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T16:29:03.365294200Z",
     "start_time": "2026-02-13T16:29:03.325080800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# BLOCK 1 — Robust Project Root + Paths\n",
    "# Finds the folder that contains \"01_Raw_Data\"\n",
    "# ==========================================================\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "while not (PROJECT_ROOT / \"01_Raw_Data\").exists():\n",
    "    if PROJECT_ROOT.parent == PROJECT_ROOT:\n",
    "        raise RuntimeError(\"Could not find PROJECT_ROOT (folder containing 01_Raw_Data).\")\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "RAW_ROOT = PROJECT_ROOT / \"01_Raw_Data\" / \"Gross Regional Domestic Product\"\n",
    "WITH_DIR = RAW_ROOT / \"By Industry (with NIR, 2025)\"\n",
    "WITHOUT_DIR = RAW_ROOT / \"By Industry (without NIR, 2021)\"\n",
    "\n",
    "OUT_DIR = PROJECT_ROOT / \"03_Cleaned_Data\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"WITH_DIR:\", WITH_DIR, \"| exists:\", WITH_DIR.exists())\n",
    "print(\"WITHOUT_DIR:\", WITHOUT_DIR, \"| exists:\", WITHOUT_DIR.exists())\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: D:\\Quelvin's Files\\DATA SCIENCE\n",
      "WITH_DIR: D:\\Quelvin's Files\\DATA SCIENCE\\01_Raw_Data\\Gross Regional Domestic Product\\By Industry (with NIR, 2025) | exists: True\n",
      "WITHOUT_DIR: D:\\Quelvin's Files\\DATA SCIENCE\\01_Raw_Data\\Gross Regional Domestic Product\\By Industry (without NIR, 2021) | exists: False\n",
      "OUT_DIR: D:\\Quelvin's Files\\DATA SCIENCE\\03_Cleaned_Data\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Block 2: List Excel Files\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T16:29:07.442545500Z",
     "start_time": "2026-02-13T16:29:07.420329600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# BLOCK 1 — Robust Project Root + Paths\n",
    "# Finds the folder that contains \"01_Raw_Data\"\n",
    "# ==========================================================\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "while not (PROJECT_ROOT / \"01_Raw_Data\").exists():\n",
    "    if PROJECT_ROOT.parent == PROJECT_ROOT:\n",
    "        raise RuntimeError(\"Could not find PROJECT_ROOT (folder containing 01_Raw_Data).\")\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "RAW_ROOT = PROJECT_ROOT / \"01_Raw_Data\" / \"Gross Regional Domestic Product\"\n",
    "WITH_DIR = RAW_ROOT / \"By Industry (with NIR, 2025)\"\n",
    "# Auto-detect \"without NIR\" folder\n",
    "WITHOUT_DIR = next(\n",
    "    d for d in RAW_ROOT.iterdir()\n",
    "    if d.is_dir() and \"without\" in d.name.lower()\n",
    ")\n",
    "\n",
    "print(\"WITHOUT_DIR FOUND:\", WITHOUT_DIR)\n",
    "\n",
    "OUT_DIR = PROJECT_ROOT / \"03_Cleaned_Data\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"WITH_DIR:\", WITH_DIR, \"| exists:\", WITH_DIR.exists())\n",
    "print(\"WITHOUT_DIR:\", WITHOUT_DIR, \"| exists:\", WITHOUT_DIR.exists())\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITHOUT_DIR FOUND: D:\\Quelvin's Files\\DATA SCIENCE\\01_Raw_Data\\Gross Regional Domestic Product\\By Industry (without NIR, 2024)\n",
      "PROJECT_ROOT: D:\\Quelvin's Files\\DATA SCIENCE\n",
      "WITH_DIR: D:\\Quelvin's Files\\DATA SCIENCE\\01_Raw_Data\\Gross Regional Domestic Product\\By Industry (with NIR, 2025) | exists: True\n",
      "WITHOUT_DIR: D:\\Quelvin's Files\\DATA SCIENCE\\01_Raw_Data\\Gross Regional Domestic Product\\By Industry (without NIR, 2024) | exists: True\n",
      "OUT_DIR: D:\\Quelvin's Files\\DATA SCIENCE\\03_Cleaned_Data\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Block 3: Text Cleaner"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T16:30:12.608972800Z",
     "start_time": "2026-02-13T16:30:12.603028400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# BLOCK 3 — Text Cleaning Utility\n",
    "# ==========================================================\n",
    "def clean_text(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).replace(\"\\u00a0\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Block 4: Header Detection"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T16:30:16.322355300Z",
     "start_time": "2026-02-13T16:30:16.312040100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# BLOCK 4 — Header Row Detection (PSA tables often start lower)\n",
    "# Detects a header row by:\n",
    "#  - keyword presence OR\n",
    "#  - many year cells in same row\n",
    "# ==========================================================\n",
    "HEADER_KEYWORDS = [\n",
    "    \"industry\", \"sector\", \"economic activity\",\n",
    "    \"kind of economic activity\", \"major industry\"\n",
    "]\n",
    "YEAR_CELL = re.compile(r\"^(19\\d{2}|20\\d{2})$\")\n",
    "\n",
    "def find_header_row(df_preview: pd.DataFrame, max_scan_rows=120) -> int | None:\n",
    "    for r in range(min(max_scan_rows, len(df_preview))):\n",
    "        row = df_preview.iloc[r].astype(str).fillna(\"\")\n",
    "        low = row.str.lower()\n",
    "\n",
    "        joined = \" | \".join(low.tolist())\n",
    "        if any(k in joined for k in HEADER_KEYWORDS):\n",
    "            return r\n",
    "\n",
    "        year_hits = sum(bool(YEAR_CELL.match(v.strip())) for v in row.tolist())\n",
    "        if year_hits >= 3:\n",
    "            return r\n",
    "\n",
    "    return None\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Block 5: Read best sheet"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T16:30:31.111350300Z",
     "start_time": "2026-02-13T16:30:31.105409300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# BLOCK 5 — Read Best Sheet from an Excel File\n",
    "# Tries each sheet, finds header row, then reads with that header.\n",
    "# ==========================================================\n",
    "def read_best_sheet(path: Path, preview_rows=80):\n",
    "    xls = pd.ExcelFile(path, engine=\"openpyxl\")\n",
    "\n",
    "    for sheet in xls.sheet_names:\n",
    "        prev = pd.read_excel(path, sheet_name=sheet, engine=\"openpyxl\", header=None, nrows=preview_rows)\n",
    "        hdr = find_header_row(prev)\n",
    "\n",
    "        if hdr is not None:\n",
    "            df = pd.read_excel(path, sheet_name=sheet, engine=\"openpyxl\", header=hdr)\n",
    "            df.columns = [clean_text(c) for c in df.columns]\n",
    "            df = df.dropna(axis=1, how=\"all\").copy()\n",
    "            return sheet, df\n",
    "\n",
    "    # fallback if nothing detected\n",
    "    df = pd.read_excel(path, sheet_name=0, engine=\"openpyxl\")\n",
    "    df.columns = [clean_text(c) for c in df.columns]\n",
    "    df = df.dropna(axis=1, how=\"all\").copy()\n",
    "    return xls.sheet_names[0], df\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Block 6: Infer Year + Price_Type from column header\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T16:30:33.305811800Z",
     "start_time": "2026-02-13T16:30:33.299695700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# BLOCK 6 — Infer Year and Price_Type from column names\n",
    "# Works for headers like:\n",
    "#   \"At Current Prices 2000\"\n",
    "#   \"2000 At Constant 2018 Prices\"\n",
    "# ==========================================================\n",
    "YEAR_RE = re.compile(r\"(19\\d{2}|20\\d{2})\")\n",
    "\n",
    "def infer_year_and_price(col_name: str):\n",
    "    c = clean_text(col_name).lower()\n",
    "    year_match = YEAR_RE.search(c)\n",
    "    year = int(year_match.group(1)) if year_match else None\n",
    "\n",
    "    if \"current\" in c:\n",
    "        price_type = \"At Current Prices\"\n",
    "    elif \"constant\" in c:\n",
    "        price_type = \"At Constant 2018 Prices\"\n",
    "    else:\n",
    "        price_type = None\n",
    "\n",
    "    return year, price_type\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Block 7: Process ONE file into long format"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T16:30:35.085716200Z",
     "start_time": "2026-02-13T16:30:35.069957700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "def standardize_region(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Standardize Region names so you don't get duplicates like:\n",
    "    'MIMAROPA' vs 'MIMAROPA Region'\n",
    "\n",
    "    RULE (chosen): Always output 'MIMAROPA Region' for MIMAROPA,\n",
    "    leave other regions as-is (but cleaned).\n",
    "    \"\"\"\n",
    "    name = clean_text(name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip()\n",
    "\n",
    "    # ✅ force MIMAROPA to a single label\n",
    "    if name.lower() in {\"mimaropa\", \"mimaropa region\"}:\n",
    "        return \"MIMAROPA Region\"\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def process_one_file(file_path: Path, regime_label: str) -> pd.DataFrame:\n",
    "    # ✅ FIXED: standardize region name right after extracting it from filename\n",
    "    region_name = standardize_region(file_path.stem.split(\",\")[0])\n",
    "\n",
    "    # --- read sheet raw (no header) ---\n",
    "    sheet, _ = read_best_sheet(file_path)  # reuse your sheet chooser\n",
    "    raw = pd.read_excel(file_path, sheet_name=sheet, engine=\"openpyxl\", header=None)\n",
    "\n",
    "    # --- find the row that contains BOTH \"At Current\" and \"At Constant\" (price header row) ---\n",
    "    def row_has_price_headers(row):\n",
    "        s = row.astype(str).str.lower().fillna(\"\")\n",
    "        joined = \" | \".join(s.tolist())\n",
    "        return (\"at current\" in joined) and (\"at constant\" in joined)\n",
    "\n",
    "    price_row = None\n",
    "    for r in range(min(60, len(raw))):\n",
    "        if row_has_price_headers(raw.iloc[r]):\n",
    "            price_row = r\n",
    "            break\n",
    "    if price_row is None:\n",
    "        raise ValueError(f\"Could not find price header row (sheet={sheet}, file={file_path.name}).\")\n",
    "\n",
    "    year_row = price_row + 1  # in your preview, years are directly below\n",
    "\n",
    "    # --- build column -> Price_Type map from the price_row ---\n",
    "    price_hdr = raw.iloc[price_row].astype(str).str.strip()\n",
    "    price_type_by_col = pd.Series(index=raw.columns, dtype=\"object\")\n",
    "\n",
    "    lower = price_hdr.str.lower()\n",
    "    price_type_by_col[lower.str.contains(\"at current\")] = \"At Current Prices\"\n",
    "    price_type_by_col[lower.str.contains(\"at constant\")] = \"At Constant 2018 Prices\"\n",
    "    price_type_by_col = price_type_by_col.ffill()  # fill across unnamed columns in each block\n",
    "\n",
    "    # --- build column -> Year map from year_row ---\n",
    "    years = raw.iloc[year_row]\n",
    "    year_by_col = pd.Series(index=raw.columns, dtype=\"float64\")\n",
    "    year_by_col[:] = pd.to_numeric(years, errors=\"coerce\")  # 2022, 2023, 2024...\n",
    "\n",
    "    # --- value columns are those that have BOTH a price type and a year ---\n",
    "    value_cols = [\n",
    "        c for c in raw.columns\n",
    "        if pd.notna(price_type_by_col.get(c)) and pd.notna(year_by_col.get(c))\n",
    "    ]\n",
    "    if len(value_cols) == 0:\n",
    "        raise ValueError(f\"No value columns found after mapping (sheet={sheet}, file={file_path.name}).\")\n",
    "\n",
    "    # --- industry rows start after year_row ---\n",
    "    data = raw.iloc[year_row + 1:].copy()\n",
    "\n",
    "    # first column that contains industry labels is the one where industries appear\n",
    "    # choose the column with the most non-null text entries\n",
    "    best_label_col = None\n",
    "    best_score = -1\n",
    "    for c in raw.columns[:3]:  # usually in first few cols\n",
    "        col = data[c].astype(str)\n",
    "        score = col.str.contains(r\"[A-Za-z]\", regex=True).sum()\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_label_col = c\n",
    "\n",
    "    label_col = best_label_col if best_label_col is not None else 0\n",
    "    data = data.rename(columns={label_col: \"Industry\"})\n",
    "\n",
    "    data[\"Industry\"] = data[\"Industry\"].apply(clean_text)\n",
    "\n",
    "    # drop rows that are empty labels\n",
    "    data = data[data[\"Industry\"].notna() & (data[\"Industry\"] != \"\")].copy()\n",
    "\n",
    "    # --- clean leading dots like \"..Industry\" / \"....Manufacturing\" ---\n",
    "    data[\"Industry\"] = data[\"Industry\"].str.replace(r\"^\\.+\\s*\", \"\", regex=True).str.strip()\n",
    "\n",
    "    # --- melt ---\n",
    "    long_df = data.melt(\n",
    "        id_vars=[\"Industry\"],\n",
    "        value_vars=value_cols,\n",
    "        var_name=\"col_id\",\n",
    "        value_name=\"GRDP\"\n",
    "    )\n",
    "\n",
    "    # attach Price_Type and Year from mapping\n",
    "    long_df[\"Price_Type\"] = long_df[\"col_id\"].map(price_type_by_col)\n",
    "    long_df[\"Year\"] = long_df[\"col_id\"].map(year_by_col).astype(\"Int64\")\n",
    "\n",
    "    # numeric cleanup\n",
    "    long_df[\"GRDP\"] = pd.to_numeric(\n",
    "        long_df[\"GRDP\"].astype(str).str.replace(\",\", \"\", regex=False),\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    long_df = long_df.dropna(subset=[\"GRDP\", \"Year\", \"Price_Type\", \"Industry\"]).copy()\n",
    "\n",
    "    # add metadata\n",
    "    long_df[\"Region\"] = region_name\n",
    "    long_df[\"GRDP_Regime\"] = regime_label\n",
    "    long_df[\"Source_File\"] = file_path.name\n",
    "\n",
    "    # final columns\n",
    "    long_df = long_df[[\"Region\", \"Industry\", \"Year\", \"Price_Type\", \"GRDP\", \"GRDP_Regime\", \"Source_File\"]].copy()\n",
    "\n",
    "    return long_df\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Block 8: Process an entire folder + write error logs to 03_Cleaned_Data"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T16:30:38.458451700Z",
     "start_time": "2026-02-13T16:30:38.450614500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# BLOCK 8 — Process Folder + Error Log\n",
    "# Saves errors to 03_Cleaned_Data so you can always find it.\n",
    "# ==========================================================\n",
    "def process_folder(folder: Path, regime_label: str, error_log_name: str) -> pd.DataFrame:\n",
    "    files = list_excel_files(folder)\n",
    "    print(f\"[INFO] {regime_label}: Found {len(files)} Excel files in {folder}\")\n",
    "\n",
    "    if len(files) == 0:\n",
    "        raise RuntimeError(f\"No Excel files found in {folder}\")\n",
    "\n",
    "    all_parts = []\n",
    "    errors = []\n",
    "\n",
    "    for fp in files:\n",
    "        try:\n",
    "            part = process_one_file(fp, regime_label)\n",
    "            all_parts.append(part)\n",
    "        except Exception as e:\n",
    "            errors.append({\"file\": fp.name, \"error\": repr(e)})\n",
    "\n",
    "    # Always write error log if anything failed\n",
    "    if errors:\n",
    "        err_df = pd.DataFrame(errors)\n",
    "        err_path = OUT_DIR / error_log_name\n",
    "        err_df.to_csv(err_path, index=False)\n",
    "        print(f\"[WARN] {len(errors)} files failed for {regime_label}. Logged to: {err_path}\")\n",
    "        print(err_df.head(5).to_string(index=False))\n",
    "\n",
    "    if not all_parts:\n",
    "        # Print one-file debug hint\n",
    "        print(\"\\n[DEBUG] Trying to inspect first file quickly...\")\n",
    "        sample = files[0]\n",
    "        print(\"Sample file:\", sample)\n",
    "        try:\n",
    "            sheet, df = read_best_sheet(sample)\n",
    "            print(\"Chosen sheet:\", sheet)\n",
    "            print(\"Columns:\", df.columns.tolist()[:20])\n",
    "            print(df.head(5).to_string(index=False))\n",
    "        except Exception as e:\n",
    "            print(\"Even sample debug failed:\", repr(e))\n",
    "\n",
    "        raise RuntimeError(f\"No files successfully processed in {folder.resolve()}\")\n",
    "\n",
    "    return pd.concat(all_parts, ignore_index=True)\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T16:30:42.639466900Z",
     "start_time": "2026-02-13T16:30:42.633863500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def list_excel_files(folder: Path) -> list[Path]:\n",
    "    folder = Path(folder)\n",
    "    files = []\n",
    "    files += list(folder.rglob(\"*.xlsx\"))\n",
    "    files += list(folder.rglob(\"*.xlsm\"))\n",
    "    files += list(folder.rglob(\"*.xls\"))   # safe addition\n",
    "    files = sorted(set(files), key=lambda p: p.name.lower())\n",
    "    return [f for f in files if f.is_file()]\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Block 9: Run ingestion (creates combined long table)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T16:30:45.597266600Z",
     "start_time": "2026-02-13T16:30:45.316182600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# BLOCK 9 — RUN INGESTION\n",
    "# ==========================================================\n",
    "with_long = process_folder(WITH_DIR, \"With NIR\", \"_preprocessing_errors_with_nir.csv\")\n",
    "without_long = process_folder(WITHOUT_DIR, \"Without NIR\", \"_preprocessing_errors_without_nir.csv\")\n",
    "\n",
    "combined = pd.concat([with_long, without_long], ignore_index=True)\n",
    "\n",
    "print(\"combined shape:\", combined.shape)\n",
    "print(\"Price types:\\n\", combined[\"Price_Type\"].value_counts())\n",
    "print(\"Year range:\", combined[\"Year\"].min(), \"to\", combined[\"Year\"].max())\n",
    "combined.head()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] With NIR: Found 17 Excel files in D:\\Quelvin's Files\\DATA SCIENCE\\01_Raw_Data\\Gross Regional Domestic Product\\By Industry (with NIR, 2025)\n",
      "[WARN] 17 files failed for With NIR. Logged to: D:\\Quelvin's Files\\DATA SCIENCE\\03_Cleaned_Data\\_preprocessing_errors_with_nir.csv\n",
      "                                                                                              file                                                                                       error\n",
      "Bangsamoro Autonomous Region in Muslim Mindanao, Gross Regional Domestic Product, by Industry.xlsm ImportError('`Import openpyxl` failed.  Use pip or conda to install the openpyxl package.')\n",
      "                                   Bicol Region, Gross Regional Domestic Product, by Industry.xlsm ImportError('`Import openpyxl` failed.  Use pip or conda to install the openpyxl package.')\n",
      "                                 Cagayan Valley, Gross Regional Domestic Product, by Industry.xlsx ImportError('`Import openpyxl` failed.  Use pip or conda to install the openpyxl package.')\n",
      "                                    CALABARZON , Gross Regional Domestic Product, by Industry.xlsx ImportError('`Import openpyxl` failed.  Use pip or conda to install the openpyxl package.')\n",
      "                                         Caraga, Gross Regional Domestic Product, by Industry.xlsx ImportError('`Import openpyxl` failed.  Use pip or conda to install the openpyxl package.')\n",
      "\n",
      "[DEBUG] Trying to inspect first file quickly...\n",
      "Sample file: D:\\Quelvin's Files\\DATA SCIENCE\\01_Raw_Data\\Gross Regional Domestic Product\\By Industry (with NIR, 2025)\\By Industry (with NIR, 2025)\\Bangsamoro Autonomous Region in Muslim Mindanao, Gross Regional Domestic Product, by Industry.xlsm\n",
      "Even sample debug failed: ImportError('`Import openpyxl` failed.  Use pip or conda to install the openpyxl package.')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No files successfully processed in D:\\Quelvin's Files\\DATA SCIENCE\\01_Raw_Data\\Gross Regional Domestic Product\\By Industry (with NIR, 2025)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# ==========================================================\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m# BLOCK 9 — RUN INGESTION\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# ==========================================================\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m with_long = \u001B[43mprocess_folder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mWITH_DIR\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mWith NIR\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m_preprocessing_errors_with_nir.csv\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m without_long = process_folder(WITHOUT_DIR, \u001B[33m\"\u001B[39m\u001B[33mWithout NIR\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33m_preprocessing_errors_without_nir.csv\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      7\u001B[39m combined = pd.concat([with_long, without_long], ignore_index=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 43\u001B[39m, in \u001B[36mprocess_folder\u001B[39m\u001B[34m(folder, regime_label, error_log_name)\u001B[39m\n\u001B[32m     40\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     41\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mEven sample debug failed:\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28mrepr\u001B[39m(e))\n\u001B[32m---> \u001B[39m\u001B[32m43\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNo files successfully processed in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfolder.resolve()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     45\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m pd.concat(all_parts, ignore_index=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[31mRuntimeError\u001B[39m: No files successfully processed in D:\\Quelvin's Files\\DATA SCIENCE\\01_Raw_Data\\Gross Regional Domestic Product\\By Industry (with NIR, 2025)"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Block 9.1: Quick sanity checks (make sure data is valid)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# BLOCK 9.1 — Sanity Checks (combined long table)\n",
    "# ==========================================================\n",
    "print(\"WITH rows:\", len(with_long))\n",
    "print(\"WITHOUT rows:\", len(without_long))\n",
    "print(\"COMBINED rows:\", len(combined))\n",
    "\n",
    "print(\"\\nPrice_Type distribution:\")\n",
    "print(combined[\"Price_Type\"].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nYear range:\", combined[\"Year\"].min(), \"to\", combined[\"Year\"].max())\n",
    "\n",
    "print(\"\\nSample rows:\")\n",
    "display(combined.head(10)) if \"display\" in globals() else print(combined.head(10).to_string(index=False))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Block 9.2: Clean \"Industry\" Labels"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# BLOCK 9.2 — Clean Industry Labels\n",
    "# ==========================================================\n",
    "combined[\"Industry\"] = combined[\"Industry\"].astype(str).str.strip()\n",
    "combined[\"Industry\"] = combined[\"Industry\"].str.replace(r\"^\\.+\\s*\", \"\", regex=True)\n",
    "combined[\"Industry\"] = combined[\"Industry\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "# Remove obvious blanks\n",
    "combined = combined[combined[\"Industry\"].notna() & (combined[\"Industry\"] != \"\")].copy()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Dataset 1 / 2 / 3 (CLEANED)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# BLOCK 10 — Build Dataset 1/2/3 (CLEANED)\n",
    "# ==========================================================\n",
    "\n",
    "def map_main_sector(industry: str):\n",
    "    s = str(industry).lower().strip()\n",
    "\n",
    "    if s == \"gross domestic product\":\n",
    "        return \"Gross Domestic Product\"\n",
    "    if \"agriculture\" in s and (\"forestry\" in s or \"fishing\" in s or \"agriculture\" in s):\n",
    "        return \"Agriculture, forestry, and fishing\"\n",
    "    if s == \"industry\" or s.endswith(\"industry\"):\n",
    "        return \"Industry\"\n",
    "    if s == \"services\" or s.endswith(\"services\"):\n",
    "        return \"Services\"\n",
    "    return None\n",
    "\n",
    "combined[\"Main_Sector\"] = combined[\"Industry\"].apply(map_main_sector)\n",
    "\n",
    "# --------------------------\n",
    "# Dataset 3\n",
    "# --------------------------\n",
    "df3 = (\n",
    "    combined[combined[\"Main_Sector\"].notna()].copy()\n",
    "    .groupby([\"Region\", \"Main_Sector\", \"Year\", \"Price_Type\"], as_index=False)[\"GRDP\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"GRDP\": \"Sector_GRDP\"})\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Dataset 2 (exclude GDP total)\n",
    "# --------------------------\n",
    "df2 = (\n",
    "    df3[df3[\"Main_Sector\"].ne(\"Gross Domestic Product\")].copy()\n",
    "    .groupby([\"Main_Sector\", \"Year\", \"Price_Type\"], as_index=False)[\"Sector_GRDP\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"Sector_GRDP\": \"Total_GRDP\"})\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Dataset 1 (prefer GDP total row if present)\n",
    "# --------------------------\n",
    "gdp_rows = combined[combined[\"Industry\"].astype(str).str.strip().eq(\"Gross Domestic Product\")].copy()\n",
    "\n",
    "if len(gdp_rows) > 0:\n",
    "    df1 = (\n",
    "        gdp_rows.groupby([\"Region\", \"Year\", \"Price_Type\"], as_index=False)[\"GRDP\"]\n",
    "        .sum()\n",
    "        .rename(columns={\"GRDP\": \"Total_GRDP\"})\n",
    "    )\n",
    "else:\n",
    "    df1 = (\n",
    "        df3[df3[\"Main_Sector\"].ne(\"Gross Domestic Product\")].copy()\n",
    "        .groupby([\"Region\", \"Year\", \"Price_Type\"], as_index=False)[\"Sector_GRDP\"]\n",
    "        .sum()\n",
    "        .rename(columns={\"Sector_GRDP\": \"Total_GRDP\"})\n",
    "    )\n",
    "\n",
    "print(\"DF1 shape:\", df1.shape)\n",
    "print(\"DF2 shape:\", df2.shape)\n",
    "print(\"DF3 shape:\", df3.shape)\n",
    "\n",
    "print(\"\\nDF1 Price_Type:\")\n",
    "print(df1[\"Price_Type\"].value_counts())\n",
    "\n",
    "print(\"\\nDF2 Main_Sector:\")\n",
    "print(df2[\"Main_Sector\"].value_counts())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Block 10.1: Validate that Dataset 2 has NO \"Gross Domestic Product\""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# BLOCK 10.1 — Validate Dataset 2 (must not include GDP total)\n",
    "# ==========================================================\n",
    "assert \"Gross Domestic Product\" not in df2[\"Main_Sector\"].unique()\n",
    "print(\"✅ Dataset 2 is clean (no GDP total row).\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# BLOCK 11 — Save Outputs (CLEANED CSVs)\n",
    "# ==========================================================\n",
    "df1.to_csv(OUT_DIR / \"Dataset 1 Regional_Economic_Magnitude (CLEANED).csv\", index=False)\n",
    "df2.to_csv(OUT_DIR / \"Dataset 2 Sector_Economic_Structure (CLEANED).csv\", index=False)\n",
    "df3.to_csv(OUT_DIR / \"Dataset 3 Region_Sector_Structure (CLEANED).csv\", index=False)\n",
    "\n",
    "print(\"✅ Saved to:\", OUT_DIR)\n",
    "print(\" - Dataset 1 Regional_Economic_Magnitude (CLEANED).csv\")\n",
    "print(\" - Dataset 2 Sector_Economic_Structure (CLEANED).csv\")\n",
    "print(\" - Dataset 3 Region_Sector_Structure (CLEANED).csv\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# BLOCK 12 — Final Quick Checks\n",
    "# ==========================================================\n",
    "print(\"\\nDF1 preview:\")\n",
    "print(df1.head(5).to_string(index=False))\n",
    "\n",
    "print(\"\\nDF2 preview:\")\n",
    "print(df2.head(5).to_string(index=False))\n",
    "\n",
    "print(\"\\nDF3 preview:\")\n",
    "print(df3.head(5).to_string(index=False))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regions = sorted(combined[\"Region\"].dropna().unique())\n",
    "print(\"Total regions:\", len(regions))\n",
    "print(\"Regions (sample):\", regions[:30])\n",
    "\n",
    "# search for likely BARMM strings\n",
    "hits = [r for r in regions if \"barmm\" in r.lower() or \"bangsamoro\" in r.lower() or \"muslim mindanao\" in r.lower()]\n",
    "print(\"BARMM-like hits:\", hits)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "files = list_excel_files(WITH_DIR)\n",
    "print(\"Total files:\", len(files))\n",
    "\n",
    "barmm_files = [\n",
    "    f.name for f in files\n",
    "    if (\"barmm\" in f.name.lower())\n",
    "    or (\"bangsamoro\" in f.name.lower())\n",
    "    or (\"armm\" in f.name.lower())\n",
    "    or (\"muslim mindanao\" in f.name.lower())\n",
    "]\n",
    "\n",
    "print(\"BARMM-like files:\", barmm_files)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
